<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>机器学习实践——多元线性回归</title>
    <url>/2021/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h2 id="数据集来源">数据集来源</h2>
<p>数据集来自Kaggle的机器学习入门题目--<a href="https://www.kaggle.com/c/titanic/overview">Titanic</a>。这道题目本来应该用分类器算法进行预测，考虑到只是用来入门，所以就用在了线性回归的实践之中，在最后的输出使用阈值法即可。<span id="more"></span></p>
<h2 id="数据预处理">数据预处理</h2>
<h3 id="读取数据-验证数据完整性">1.读取数据 &amp; 验证数据完整性</h3>
<p>数据不一定都是完整的，可能会有空数据的存在：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">train = pd.read_csv(<span class="string">&#x27;data/train.csv&#x27;</span>) </span><br><span class="line"><span class="comment"># 查看数据信息</span></span><br><span class="line"><span class="built_in">print</span>(train.info())</span><br></pre></td></tr></table></figure>
<h3 id="填充空数据">2.填充空数据</h3>
<p>经过上一步的验证得知，Titanic训练集中，年龄、舱号和登船地点三项存在空数据，舱号都以类似于“C23”、“D45”这种形式出现，我认为对于最终预测结果帮助不大，不会使用舱号作为属性，故只需考虑填充年龄和登船地点中的空数据。此处我采用了<strong>中位数</strong>作为年龄填充的依据，<strong>众数</strong>作为登船地点填充的依据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 年龄填充</span></span><br><span class="line">train[<span class="string">&#x27;Age&#x27;</span>] = train[<span class="string">&#x27;Age&#x27;</span>].fillna(train[<span class="string">&#x27;Age&#x27;</span>].median()) </span><br><span class="line"><span class="comment"># 登船地点填充</span></span><br><span class="line">emb = train[<span class="string">&#x27;Embarked&#x27;</span>].value_counts()</span><br><span class="line">fillstr = emb.idxmax(axis=<span class="number">1</span>)</span><br><span class="line">train[<span class="string">&#x27;Embarked&#x27;</span>] = train[<span class="string">&#x27;Embarked&#x27;</span>].fillna(fillstr)</span><br></pre></td></tr></table></figure>
<h3 id="数据数字化表示">3.数据数字化表示</h3>
<p>数据集中，性别和登船地点是使用字符记录的，所以我们需要将其转换为数字表示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 性别数字化表示</span></span><br><span class="line">train.loc[train[<span class="string">&quot;Sex&quot;</span>] == <span class="string">&quot;male&quot;</span>, <span class="string">&quot;Sex&quot;</span>] = <span class="number">0</span></span><br><span class="line">train.loc[train[<span class="string">&quot;Sex&quot;</span>] == <span class="string">&quot;female&quot;</span>, <span class="string">&quot;Sex&quot;</span>] = <span class="number">1</span></span><br><span class="line"><span class="comment"># 登船地点数字化表示</span></span><br><span class="line">train.loc[train[<span class="string">&#x27;Embarked&#x27;</span>] == <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>] = <span class="number">0</span></span><br><span class="line">train.loc[train[<span class="string">&#x27;Embarked&#x27;</span>] == <span class="string">&#x27;Q&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>] = <span class="number">1</span></span><br><span class="line">train.loc[train[<span class="string">&#x27;Embarked&#x27;</span>] == <span class="string">&#x27;S&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>] = <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="特征缩放特征归一化">4.特征缩放（特征归一化）</h3>
<p>为了不影响预测结果，需要对不同特征的数据进行归一化处理，特征归一化的公式如下： <span class="math display">\[
x_{n}=\frac{x_{n}-\mu _{n}}{s_{n}}
\]</span> 其中， <span class="math inline">\(\mu _{n}\)</span>​​​​代表平均值， $ s_{n} $​​​​​代表标准差，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 选择需要的特征列（具有主观性）</span></span><br><span class="line">x = train[[<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Age&quot;</span>, <span class="string">&quot;Parch&quot;</span>, <span class="string">&quot;Fare&quot;</span>, <span class="string">&quot;Sex&quot;</span>, <span class="string">&quot;Embarked&quot;</span>]]</span><br><span class="line"><span class="comment"># 计算平均值、标准差</span></span><br><span class="line">avg = np.zeros(x.shape[<span class="number">1</span>])</span><br><span class="line">std = np.zeros(x.shape[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 特征归一化</span></span><br><span class="line">x_norm = (x - avg) / std</span><br></pre></td></tr></table></figure>
<h2 id="代价函数">代价函数</h2>
<p>多元线性回归的代价函数定义如下： <span class="math display">\[
J\left(\theta_{0}, \theta_{1} \ldots \theta_{n}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
\]</span></p>
<p><span class="math display">\[
h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}
\]</span></p>
<p>其中，上角标代表第<span class="math inline">\(i\)</span>​个数据，下角标代表每个数据的第$ j $​个属性。</p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">x, y, theta</span>):</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>] <span class="comment"># 数据个数</span></span><br><span class="line">    cost = <span class="number">1</span>/<span class="number">2</span>*m*np.<span class="built_in">sum</span>(np.power(x.dot(theta) - y, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<h2 id="梯度下降">梯度下降</h2>
<p>梯度下降的更新公式如下： <span class="math display">\[
\theta_{\mathrm{j}}:=\theta_{\mathrm{j}}-\alpha \frac{\partial}{\partial \theta_{\mathrm{j}}} \frac{1}{2 \mathrm{~m}} \sum_{\mathrm{i}=1}^{\mathrm{m}}\left(\mathrm{h}_{\theta}\left(\mathrm{x}^{(\mathrm{i})}\right)-\mathrm{y}^{(\mathrm{i})}\right)^{2}
\]</span> 求导数后得到： <span class="math display">\[
\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x_{j}^{(i)}\right)
\]</span> 代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span>(<span class="params">x, y, theta, alpha, num_iters</span>):</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>] <span class="comment"># 数据个数</span></span><br><span class="line">    cost_history = np.zeros(num_iters) <span class="comment"># 记录每次梯度下降后的误差</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        theta = theta - alpha/m*np.dot(x.T, (x.dot(theta) - y))</span><br><span class="line">        cost_history[i] = cost(x, y, theta)</span><br><span class="line">    <span class="keyword">return</span> theta, cost_history</span><br></pre></td></tr></table></figure>
<h2 id="开始训练">开始训练</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">alpha = <span class="number">0.2</span> <span class="comment"># 学习率</span></span><br><span class="line">iters = <span class="number">100</span> <span class="comment">#迭代次数</span></span><br><span class="line">[theta, cost_history] = gradientDescent(x_norm, y_train, theta, alpha, iters)</span><br></pre></td></tr></table></figure>
<h2 id="保存结果">保存结果</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = x_norm.dot(theta)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(result)): <span class="comment"># 使用阈值法将结果二值化，以满足二分类的需求</span></span><br><span class="line">    <span class="keyword">if</span> result[i] &gt;= <span class="number">0.5</span>:</span><br><span class="line">        result[i] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result[i] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h2 id="计算模型在训练集上的准确率">计算模型在训练集上的准确率</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_correct = <span class="number">0</span> <span class="comment"># 统计预测正确个数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(result)):</span><br><span class="line">    <span class="keyword">if</span> result[i] == y[i]:</span><br><span class="line">        num_correct += <span class="number">1</span></span><br><span class="line">accuracy_train = num_correct / <span class="built_in">len</span>(result)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;在训练集上的正确率为：&#x27;</span>+<span class="built_in">str</span>(accuracy_train))</span><br></pre></td></tr></table></figure>
<h2 id="测试集数据预处理归一化">测试集数据预处理+归一化</h2>
<p>方法同训练集。</p>
<h2 id="保存预测结果-导出kaggle要求的格式的文件">保存预测结果 &amp; 导出Kaggle要求的格式的文件</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = x_test.dot(theta) <span class="comment"># 保存结果</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(result)):</span><br><span class="line">    <span class="keyword">if</span> result[i] &gt;= <span class="number">0.5</span>:</span><br><span class="line">        result[i] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result[i] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">passengerId = test[<span class="string">&#x27;PassengerId&#x27;</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>) <span class="comment"># 导出文件</span></span><br><span class="line"><span class="built_in">list</span> = np.hstack((passengerId, result))</span><br><span class="line">column = [<span class="string">&#x27;PassengerId&#x27;</span>, <span class="string">&#x27;Survived&#x27;</span>]</span><br><span class="line">resultFile = pd.DataFrame(columns=column, data=<span class="built_in">list</span>)</span><br><span class="line">resultFile.to_csv(<span class="string">&#x27;data/submission.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>完整代码及数据请<a href="https://github.com/Ubivision/ML-ALgorithms">点击这里</a></p>
<h2 id="总结">总结</h2>
<p>经过上述过程，最终在训练集上准确率为79.3%，在测试集上准确率为76.8%。可以改进之处我觉得有两个方面，一方面是对于特征的分析和选择，另一方面就是方法，使用专门的分类器算法势必会让准确率有所提升。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类问题</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo无法正确显示LaTeX数学公式的解决方法</title>
    <url>/2021/07/26/Hexo%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E6%98%BE%E7%A4%BALaTeX%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="起因">起因</h2>
<p>昨天晚上写好了第一篇博文，准备在本地看看效果，想的没啥问题就直接deploy了，好家伙公式一个都显示不对。我在网上搜寻了一下原因，大致意思就是：Hexo自带的Markdown渲染器会使得LaTeX与Markdown冲突，从而使得一些LaTeX书写的内容不能正确转义，造成数学公式无法正确显示。<span id="more"></span></p>
<h2 id="解决方法">解决方法</h2>
<p>我搜寻了许多解决方案，不过归根到底，就是要更换Hexo自带的Markdown渲染器。关于Hexo的几种渲染器的介绍，我找到了一篇比较详细的，<a href="https://bugwz.com/2019/09/17/hexo-markdown-renderer/#%E4%B8%80-%E5%BC%95%E8%A8%80">在这里</a>。然后网上的大部分方法是：更换渲染器为kramed，再更改一下此渲染器的转义规则，就OK了。不过我在尝试了上述方法后，发现并不行，行内公式成功显示了，公式块内的公式却直接消失了，我也不知道是什么原因。在试错了多种方法之后，我终于找到了一种适用于我的解决方案，那就是pandoc渲染器：</p>
<h3 id="卸载hexo自带渲染器">1.卸载Hexo自带渲染器</h3>
<p>在博客的根目录下，右键--Git Bash--执行如下命令：（下同）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked</span><br></pre></td></tr></table></figure>
<h3 id="安装pandoc">2.安装pandoc</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-renderer-pandoc</span><br></pre></td></tr></table></figure>
<p>与其他渲染器不同的是，pandoc直接用npm进行安装后无法使用，需要再通过手动下载安装包安装一个pandoc的发行版后，才可以正常使用，<a href="https://github.com/jgm/pandoc/releases">链接在此</a>。如果是Windows系统的话，选择这个：</p>
<img src="/2021/07/26/Hexo%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E6%98%BE%E7%A4%BALaTeX%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/pandoc.jpg" class>
<p>安装完成后，不出意外就可以使用了。</p>
<h2 id="存在的bug-解决方法">存在的bug &amp; 解决方法</h2>
<p>使用Pandoc作为渲染器虽然解决了公式块的显示问题，但是行内公式的显示依然存在问题，我在误打误撞中发现，行内公式的一般格式为：​</p>
<img src="/2021/07/26/Hexo%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E6%98%BE%E7%A4%BALaTeX%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/fomula-vanila.jpg" class>
<p>可能因为我是从数学公式公式转换LaTeX的网站上直接复制的代码，在公式的前后，都有一个空格字符，问题就出在这里，如果去掉这两个空格，行内公式的渲染就不会有任何问题（至少我目前没有发现），也就是这样：</p>
<img src="/2021/07/26/Hexo%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E6%98%BE%E7%A4%BALaTeX%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/fomula-fix.jpg" class>
<p>这样写之后，再通过pandoc渲染，就没有问题了。</p>
]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实践——逻辑回归</title>
    <url>/2021/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h2 id="概述">概述</h2>
<p>本篇主要的目的是：从实践角度去理解逻辑回归的过程（其中的公式、表达式均以向量化的形式出现），数据集依旧使用Titanic。<span id="more"></span></p>
<h2 id="逻辑回归与线性回归的联系">逻辑回归与线性回归的联系</h2>
<p>逻辑回归模型，其实就是在线性回归的基础上，套了一个Sigmoid函数，换句话说：逻辑回归模型的表达式是一个两层复合函数，其中内层是线性回归模型，外层是Sigmoid函数。通过这种做法，使得线性回归的输出被压缩在0到1之间，以便完成二分类任务。</p>
<p><span class="math display">\[
\begin{aligned}
Sigmoid函数：H &amp;=\frac{1}{1+e^{-Z}} \\
线性模型：Z &amp;=X W
\end{aligned}
\]</span> 其中的<span class="math inline">\(X\)</span>​​​的维度是(m, n+1)(n代表特征个数，m代表数据个数，下同)，<span class="math inline">\(W\)</span>​​​​​​​​的维度是(n+1, 1)，n+1中“1”的含义是：偏置b。所以<span class="math inline">\(z\)</span>​​​的维度是(m, 1)，<span class="math inline">\(H\)</span>​​​不涉及矩阵间运算，故维度也是(m, 1)</p>
<h2 id="逻辑回归代价函数的理解">逻辑回归代价函数的理解</h2>
<p>不论是什么模型，我们都希望代价函数能够做到：在模型预测正确的时候代价函数值为0，预测错误的时候代价函数值为错误值和真实值的差距。但是在逻辑回归中，因为解决的是二分类问题，是离散的，结果只有正反两种，如果继续采用MSE作为代价函数，那么在预测错误时所反应出的“有多错”，就不够严重，换句话说，MSE在二分类问题中对待预测错误时的惩罚力度不够。所以这时候就需要另一种代价函数，对待预测错误时能够狠狠地进行惩罚，预测结果越接近真实值，惩罚力度就越小，所以在逻辑回归中，代价函数是以对数函数的形式出现的，可以分为两种情况讨论。</p>
<p>当真实值为1时，此时代价函数为： <span class="math display">\[
Cost=-ln(H)
\]</span> 图像为：</p>
<img src="/2021/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E7%9C%9F%E5%AE%9E%E5%80%BC%E4%B8%BA1.jpg" class>
<p>当真实值为0时，此时代价函数为： <span class="math display">\[
Cost=-ln(1-H)
\]</span> 图像为：</p>
<img src="/2021/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E7%9C%9F%E5%AE%9E%E5%80%BC%E4%B8%BA0.jpg" class>
<p>将两种情况结合起来考虑，就得到了逻辑回归的代价函数： <span class="math display">\[
Cost=-Yln(H)-(1-Y)ln(1-H)
\]</span> 当然，定义中的代价函数还有一个常数系数1/m，因为上述内容是在阐述原理，所以没有加。</p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">x, y, theta</span>):</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    z = x.dot(theta)</span><br><span class="line">    h = <span class="number">1</span> / (np.exp(-z) + <span class="number">1</span>)</span><br><span class="line">    cost = -<span class="number">1</span>/m*np.<span class="built_in">sum</span>((y*np.log(h)+(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-h)))</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<h2 id="梯度下降">梯度下降</h2>
<h3 id="求偏导">1.求偏导</h3>
<p>思路还是那个思路，对代价函数中的参数求偏导得到梯度，然后参数进行反梯度方向的更新。故首先要做的就是对代价函数求导。观察代价函数可以发现，如果要对参数<span class="math inline">\(w\)</span>​​进行求导，其实就是进行复合函数求导： <span class="math display">\[
\begin{gathered}
\frac{d C o s t}{d W}=\frac{d C}{d H} * \frac{d H}{d Z} * \frac{d Z}{d W} \\
=X^{T} *\left\{\left(-\frac{Y}{H}-\frac{1-Y}{1-H} * -1\right) * H(1-H)\right\} \\
=X^{T} *\left\{\left(\frac{\left(-Y(1-H)+H\left(1-Y\right)\right)}{H(1-H)}\right) * H(1-H)\right\} \\
=X^{T}\left\{\left(-Y+Y H+H-H Y\right)\right\} \\
=X^{T}\left(H-Y\right)
\end{gathered}
\]</span> 上述运算是直接基于矩阵的运算，乍一看可能非常劝退，但是细看其实并不复杂，我们一部分一部分拆开来看： <span class="math display">\[
\begin{gathered}
Cost=-Yln(H)-(1-Y)ln(1-H)\\
\frac{d C}{d H}=(-\frac{Y}{H}-\frac{1-Y}{1-H} *-1)
\end{gathered}
\]</span> 这部分是代价函数对Sigomid函数<span class="math inline">\(H\)</span>求导，涉及到<span class="math inline">\(lnx\)</span>​​​的导数和复合函数求导法则。 <span class="math display">\[
\begin{gathered}
H=\frac{1}{1+e^{-Z}} \\
\frac{d H}{d Z}=\frac{-e^{-Z}*-1}{\left(1+e^{-Z}\right)^{2}}=\frac{1+e^{-Z}-1}{\left(1+e^{-Z}\right)^{2}}=\frac{1}{1+e^{-Z}}\left(1-\frac{1}{1+e^{-Z}}\right)=H(1-H)
\end{gathered}
\]</span> 这部分是Sigmoid函数对线性回归模型<span class="math inline">\(Z\)</span>求导，涉及到分式求导法则和<span class="math inline">\(e^{-x}\)</span>​​​的导数。 <span class="math display">\[
\begin{gathered}
Z=XW\\
\frac{d Z}{d W}=X
\end{gathered}
\]</span> 这部分是线性回归模型<span class="math inline">\(Z\)</span>对参数<span class="math inline">\(W\)</span>的导数。</p>
<p>在这些都清楚了之后，剩下的步骤就是运算了，没有什么理解上的难度。</p>
<p>这时候我们再回过头看化简过后的结果： <span class="math display">\[
\begin{gathered}
\frac{d C o s t}{d W}=X^{T}(H-Y) \\
代入H：\\
\frac{d C o s t}{d W}=X^{T}\left(\frac{1}{1+e^{-X W}}-Y\right)
\end{gathered}
\]</span> 可能在这里会有疑问，上面才解释过<span class="math inline">\(Z\)</span>​对参数<span class="math inline">\(W\)</span>​的导数是<span class="math inline">\(X\)</span>​，这里怎么变成了<span class="math inline">\(X^{T}\)</span>​？而且本来应该右乘，这里为什么变成了左乘？这里先分析一下<span class="math inline">\(H-Y\)</span>​的维度，<span class="math inline">\(H\)</span>​由上述内容内容可知，<span class="math inline">\(H\)</span>​的维度是(m, 1)，<span class="math inline">\(Y\)</span>​的维度也是(m, 1)，所以<span class="math inline">\(H-Y\)</span>​的维度自然也是(m, 1)。<span class="math inline">\(X\)</span>​的维度是(m, n+1)，如果这时候直接右乘<span class="math inline">\(X\)</span>​，是无法相乘的，所以需要转置且变为左乘，这时候<span class="math inline">\(\frac{d C o s t}{d W}\)</span>​​​​​的维度是(n+1, 1)，n+1也正好是参数的个数。</p>
<h3 id="参数更新">2. 参数更新</h3>
<p><span class="math display">\[
W=W-alpha*\frac{d C o s t}{d W}
\]</span></p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span>(<span class="params">x, y, theta, alpha, num_iters</span>):</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    cost_history = np.zeros(num_iters)</span><br><span class="line">    z = x.dot(theta)</span><br><span class="line">    h = <span class="number">1</span> / (np.exp(-z) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        theta = theta - alpha/m*np.dot(x.T, (h-y))</span><br><span class="line">        cost_history[i] = cost(x, y, theta)</span><br><span class="line">    <span class="keyword">return</span> theta, cost_history</span><br></pre></td></tr></table></figure>
<p>完整代码及数据请<a href="https://github.com/Ubivision/ML-ALgorithms">点击这里</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类问题</tag>
      </tags>
  </entry>
</search>
