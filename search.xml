<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hexo无法正确显示LaTeX数学公式的解决方法</title>
    <url>/2021/07/26/Hexo%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E6%98%BE%E7%A4%BALaTeX%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="起因">起因</h2>
<p>昨天晚上写好了第一篇博文，准备在本地看看效果，想的没啥问题就直接deploy了，好家伙公式一个都显示不对。我在网上搜寻了一下原因，大致意思就是：Hexo自带的Markdown渲染器会使得LaTeX与Markdown冲突，从而使得一些LaTeX书写的内容不能正确转义，造成数学公式无法正确显示。<span id="more"></span></p>
<h2 id="解决方法">解决方法</h2>
<p>我搜寻了许多解决方案，不过归根到底，就是要更换Hexo自带的Markdown渲染器。关于Hexo的几种渲染器的介绍，我找到了一篇比较详细的，<a href="https://bugwz.com/2019/09/17/hexo-markdown-renderer/#%E4%B8%80-%E5%BC%95%E8%A8%80">在这里</a>。然后网上的大部分方法是：更换渲染器为kramed，再更改一下此渲染器的转义规则，就OK了。不过我在尝试了上述方法后，发现并不行，行内公式成功显示了，公式块内的公式却直接消失了，我也不知道是什么原因。在试错了多种方法之后，我终于找到了一种适用于我的解决方案，那就是pandoc渲染器：</p>
<h3 id="卸载hexo自带渲染器">1.卸载Hexo自带渲染器</h3>
<p>在博客的根目录下，右键--Git Bash--执行如下命令：（下同）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked</span><br></pre></td></tr></table></figure>
<h3 id="安装pandoc">2.安装pandoc</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-renderer-pandoc</span><br></pre></td></tr></table></figure>
<p>与其他渲染器不同的是，pandoc直接用npm进行安装后无法使用，需要再通过手动下载安装包安装一个pandoc的发行版后，才可以正常使用，<a href="https://github.com/jgm/pandoc/releases">链接在此</a>。如果是Windows系统的话，选择这个：</p>
<img src="/2021/07/26/Hexo%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E6%98%BE%E7%A4%BALaTeX%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/pandoc.jpg" class>
<p>安装完成后，不出意外就可以使用了。</p>
<h2 id="存在的bug-解决方法">存在的bug &amp; 解决方法</h2>
<p>使用Pandoc作为渲染器虽然解决了公式块的显示问题，但是行内公式的显示依然存在问题，我在误打误撞中发现，行内公式的一般格式为：​</p>
<img src="/2021/07/26/Hexo%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E6%98%BE%E7%A4%BALaTeX%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/fomula-vanila.jpg" class>
<p>可能因为我是从数学公式公式转换LaTeX的网站上直接复制的代码，在公式的前后，都有一个空格字符，问题就出在这里，如果去掉这两个空格，行内公式的渲染就不会有任何问题（至少我目前没有发现），也就是这样：</p>
<img src="/2021/07/26/Hexo%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E6%98%BE%E7%A4%BALaTeX%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/fomula-fix.jpg" class>
<p>这样写之后，再通过pandoc渲染，就没有问题了。</p>
]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实践——Kmeans</title>
    <url>/2021/07/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94Kmeans/</url>
    <content><![CDATA[<h2 id="概述">概述</h2>
<p>Kmeans是一种无监督聚类算法，试图从没有label的数据中找寻数据的潜在关系，其算法步骤如下：<span id="more"></span></p>
<p>1.随机从数据集中选取k个样本作为中心点，代表k个簇（cluster）；</p>
<p>2.遍历数据集中的每个点，计算其与每个中心点之间的距离，并将其归为与其距离最近的那个簇；</p>
<p>3.根据当前分类情况更新中心点的位置；</p>
<p>4.重复2、3步，直到达到某一条件（迭代次数、中心点位置不再改变等）。</p>
<h2 id="实践">实践</h2>
<h3 id="创建kmeans类">创建Kmeans类</h3>
<p>主要有3个函数，构造函数负责传递超参数（分为k类、迭代次数），fit函数负责根据给定数据进行分类，predict函数负责在根据给定数据已经完成分类的基础上，对新给定的数据进行类别判别。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Kmeans</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, k, max_iter</span>)：</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">fit</span>(<span class="params">self, data</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, p_data</span>):</span>             </span><br></pre></td></tr></table></figure>
<h4 id="构造函数">构造函数</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, k=<span class="number">2</span>, max_iter=<span class="number">300</span></span>):</span></span><br><span class="line">    self.k = k</span><br><span class="line">    self.max_iter = max_iter</span><br></pre></td></tr></table></figure>
<h4 id="fit函数">fit函数</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, data</span>):</span></span><br><span class="line">    self.centers = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.k):</span><br><span class="line">        self.centers[i] = data[i]   </span><br><span class="line">        <span class="comment"># 设置k个中心点，挑选输入数据点的前k个作为暂时的中心点</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">        self.clf = &#123;&#125;   <span class="comment"># 存放属于不同类别的数据点</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.k):   <span class="comment"># 创建k个位置，用index代指某个簇</span></span><br><span class="line">            self.clf[i] = []</span><br><span class="line">        <span class="keyword">for</span> feature <span class="keyword">in</span> data:</span><br><span class="line">            distances = []	<span class="comment"># 存放每个数据点到所以中心点的距离</span></span><br><span class="line">            <span class="keyword">for</span> center <span class="keyword">in</span> self.centers:</span><br><span class="line">                distances.append(np.linalg.norm(feature-self.centers[center]))</span><br><span class="line">            classification = distances.index(<span class="built_in">min</span>(distances))    </span><br><span class="line">            <span class="comment"># 记录距离其最小的簇</span></span><br><span class="line">            self.clf[classification].append(feature)    </span><br><span class="line">            <span class="comment"># 将当前数据点分类至与其距离最小的簇</span></span><br><span class="line">        prev_centers = <span class="built_in">dict</span>(self.centers)   </span><br><span class="line">        <span class="comment"># 一次分类完成，重新计算当前分类情况下每个簇的中心点</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> self.clf:</span><br><span class="line">            self.centers[c] = np.average(self.clf[c], axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="predict函数">predict函数</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, p_data</span>):</span></span><br><span class="line">    <span class="keyword">for</span> center <span class="keyword">in</span> self.centers:	<span class="comment"># 计算输入数据点与每个簇的距离</span></span><br><span class="line">    	distances = [np.linalg.norm(p_data - self.centers[center])]</span><br><span class="line">    index = distances.index(<span class="built_in">min</span>(distances))</span><br><span class="line">    <span class="keyword">return</span> index</span><br></pre></td></tr></table></figure>
<h3 id="给出数据点">给出数据点</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1.5</span>, <span class="number">1.8</span>], [<span class="number">5</span>, <span class="number">8</span>], [<span class="number">8</span>, <span class="number">8</span>], [<span class="number">1</span>, <span class="number">0.6</span>], [<span class="number">9</span>, <span class="number">11</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="进行kmeans聚类绘图">进行Kmeans聚类、绘图</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k_means = Kmeans(k=<span class="number">2</span>)</span><br><span class="line">k_means.fit(x)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(k_means.centers)):  <span class="comment"># 用五角星绘制中心点</span></span><br><span class="line">    pyplot.scatter(k_means.centers[i][<span class="number">0</span>], k_means.centers[i][<span class="number">1</span>], marker=<span class="string">&#x27;*&#x27;</span>, s=<span class="number">150</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(k_means.clf)):</span><br><span class="line">    <span class="keyword">for</span> point <span class="keyword">in</span> k_means.clf[i]:  <span class="comment"># 用圆点绘制数据点</span></span><br><span class="line">        pyplot.scatter(point[<span class="number">0</span>], point[<span class="number">1</span>], c=(<span class="string">&#x27;r&#x27;</span> <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;b&#x27;</span>))</span><br><span class="line"></span><br><span class="line">predict = [[<span class="number">2</span>, <span class="number">1</span>], [<span class="number">6</span>, <span class="number">9</span>]]  <span class="comment"># 给出新的数据点，进行预测并用x绘制</span></span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> predict:</span><br><span class="line">    cat = k_means.predict(predict)</span><br><span class="line">    pyplot.scatter(feature[<span class="number">0</span>], feature[<span class="number">1</span>], c=(<span class="string">&#x27;r&#x27;</span> <span class="keyword">if</span> cat == <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;b&#x27;</span>), marker=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line"></span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure>
<p>完整代码请<a href="https://github.com/Ubivision/ML-ALgorithms">点击这里</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实践——多元线性回归</title>
    <url>/2021/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h2 id="数据集来源">数据集来源</h2>
<p>数据集来自Kaggle的机器学习入门题目--<a href="https://www.kaggle.com/c/titanic/overview">Titanic</a>。这道题目本来应该用分类器算法进行预测，考虑到只是用来入门，所以就用在了线性回归的实践之中，在最后的输出使用阈值法即可。<span id="more"></span></p>
<h2 id="数据预处理">数据预处理</h2>
<h3 id="读取数据-验证数据完整性">1.读取数据 &amp; 验证数据完整性</h3>
<p>数据不一定都是完整的，可能会有空数据的存在：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">train = pd.read_csv(<span class="string">&#x27;data/train.csv&#x27;</span>) </span><br><span class="line"><span class="comment"># 查看数据信息</span></span><br><span class="line"><span class="built_in">print</span>(train.info())</span><br></pre></td></tr></table></figure>
<h3 id="填充空数据">2.填充空数据</h3>
<p>经过上一步的验证得知，Titanic训练集中，年龄、舱号和登船地点三项存在空数据，舱号都以类似于“C23”、“D45”这种形式出现，我认为对于最终预测结果帮助不大，不会使用舱号作为属性，故只需考虑填充年龄和登船地点中的空数据。此处我采用了<strong>中位数</strong>作为年龄填充的依据，<strong>众数</strong>作为登船地点填充的依据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 年龄填充</span></span><br><span class="line">train[<span class="string">&#x27;Age&#x27;</span>] = train[<span class="string">&#x27;Age&#x27;</span>].fillna(train[<span class="string">&#x27;Age&#x27;</span>].median()) </span><br><span class="line"><span class="comment"># 登船地点填充</span></span><br><span class="line">emb = train[<span class="string">&#x27;Embarked&#x27;</span>].value_counts()</span><br><span class="line">fillstr = emb.idxmax(axis=<span class="number">1</span>)</span><br><span class="line">train[<span class="string">&#x27;Embarked&#x27;</span>] = train[<span class="string">&#x27;Embarked&#x27;</span>].fillna(fillstr)</span><br></pre></td></tr></table></figure>
<h3 id="数据数字化表示">3.数据数字化表示</h3>
<p>数据集中，性别和登船地点是使用字符记录的，所以我们需要将其转换为数字表示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 性别数字化表示</span></span><br><span class="line">train.loc[train[<span class="string">&quot;Sex&quot;</span>] == <span class="string">&quot;male&quot;</span>, <span class="string">&quot;Sex&quot;</span>] = <span class="number">0</span></span><br><span class="line">train.loc[train[<span class="string">&quot;Sex&quot;</span>] == <span class="string">&quot;female&quot;</span>, <span class="string">&quot;Sex&quot;</span>] = <span class="number">1</span></span><br><span class="line"><span class="comment"># 登船地点数字化表示</span></span><br><span class="line">train.loc[train[<span class="string">&#x27;Embarked&#x27;</span>] == <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>] = <span class="number">0</span></span><br><span class="line">train.loc[train[<span class="string">&#x27;Embarked&#x27;</span>] == <span class="string">&#x27;Q&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>] = <span class="number">1</span></span><br><span class="line">train.loc[train[<span class="string">&#x27;Embarked&#x27;</span>] == <span class="string">&#x27;S&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>] = <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="特征缩放特征归一化">4.特征缩放（特征归一化）</h3>
<p>为了不影响预测结果，需要对不同特征的数据进行归一化处理，特征归一化的公式如下： <span class="math display">\[
x_{n}=\frac{x_{n}-\mu _{n}}{s_{n}}
\]</span> 其中， <span class="math inline">\(\mu _{n}\)</span>​​​​代表平均值， $ s_{n} $​​​​​代表标准差，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 选择需要的特征列（具有主观性）</span></span><br><span class="line">x = train[[<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Age&quot;</span>, <span class="string">&quot;Parch&quot;</span>, <span class="string">&quot;Fare&quot;</span>, <span class="string">&quot;Sex&quot;</span>, <span class="string">&quot;Embarked&quot;</span>]]</span><br><span class="line"><span class="comment"># 计算平均值、标准差</span></span><br><span class="line">avg = np.zeros(x.shape[<span class="number">1</span>])</span><br><span class="line">std = np.zeros(x.shape[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 特征归一化</span></span><br><span class="line">x_norm = (x - avg) / std</span><br></pre></td></tr></table></figure>
<h2 id="代价函数">代价函数</h2>
<p>多元线性回归的代价函数定义如下： <span class="math display">\[
J\left(\theta_{0}, \theta_{1} \ldots \theta_{n}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
\]</span></p>
<p><span class="math display">\[
h_{\theta}(x)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}
\]</span></p>
<p>其中，上角标代表第<span class="math inline">\(i\)</span>​个数据，下角标代表每个数据的第$ j $​个属性。</p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">x, y, theta</span>):</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>] <span class="comment"># 数据个数</span></span><br><span class="line">    cost = <span class="number">1</span>/<span class="number">2</span>*m*np.<span class="built_in">sum</span>(np.power(x.dot(theta) - y, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<h2 id="梯度下降">梯度下降</h2>
<p>梯度下降的更新公式如下： <span class="math display">\[
\theta_{\mathrm{j}}:=\theta_{\mathrm{j}}-\alpha \frac{\partial}{\partial \theta_{\mathrm{j}}} \frac{1}{2 \mathrm{~m}} \sum_{\mathrm{i}=1}^{\mathrm{m}}\left(\mathrm{h}_{\theta}\left(\mathrm{x}^{(\mathrm{i})}\right)-\mathrm{y}^{(\mathrm{i})}\right)^{2}
\]</span> 求导数后得到： <span class="math display">\[
\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x_{j}^{(i)}\right)
\]</span> 代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span>(<span class="params">x, y, theta, alpha, num_iters</span>):</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>] <span class="comment"># 数据个数</span></span><br><span class="line">    cost_history = np.zeros(num_iters) <span class="comment"># 记录每次梯度下降后的误差</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        theta = theta - alpha/m*np.dot(x.T, (x.dot(theta) - y))</span><br><span class="line">        cost_history[i] = cost(x, y, theta)</span><br><span class="line">    <span class="keyword">return</span> theta, cost_history</span><br></pre></td></tr></table></figure>
<h2 id="开始训练">开始训练</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">alpha = <span class="number">0.2</span> <span class="comment"># 学习率</span></span><br><span class="line">iters = <span class="number">100</span> <span class="comment">#迭代次数</span></span><br><span class="line">[theta, cost_history] = gradientDescent(x_norm, y_train, theta, alpha, iters)</span><br></pre></td></tr></table></figure>
<h2 id="保存结果">保存结果</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = x_norm.dot(theta)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(result)): <span class="comment"># 使用阈值法将结果二值化，以满足二分类的需求</span></span><br><span class="line">    <span class="keyword">if</span> result[i] &gt;= <span class="number">0.5</span>:</span><br><span class="line">        result[i] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result[i] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h2 id="计算模型在训练集上的准确率">计算模型在训练集上的准确率</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_correct = <span class="number">0</span> <span class="comment"># 统计预测正确个数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(result)):</span><br><span class="line">    <span class="keyword">if</span> result[i] == y[i]:</span><br><span class="line">        num_correct += <span class="number">1</span></span><br><span class="line">accuracy_train = num_correct / <span class="built_in">len</span>(result)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;在训练集上的正确率为：&#x27;</span>+<span class="built_in">str</span>(accuracy_train))</span><br></pre></td></tr></table></figure>
<h2 id="测试集数据预处理归一化">测试集数据预处理+归一化</h2>
<p>方法同训练集。</p>
<h2 id="保存预测结果-导出kaggle要求的格式的文件">保存预测结果 &amp; 导出Kaggle要求的格式的文件</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result = x_test.dot(theta) <span class="comment"># 保存结果</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(result)):</span><br><span class="line">    <span class="keyword">if</span> result[i] &gt;= <span class="number">0.5</span>:</span><br><span class="line">        result[i] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result[i] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">passengerId = test[<span class="string">&#x27;PassengerId&#x27;</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>) <span class="comment"># 导出文件</span></span><br><span class="line"><span class="built_in">list</span> = np.hstack((passengerId, result))</span><br><span class="line">column = [<span class="string">&#x27;PassengerId&#x27;</span>, <span class="string">&#x27;Survived&#x27;</span>]</span><br><span class="line">resultFile = pd.DataFrame(columns=column, data=<span class="built_in">list</span>)</span><br><span class="line">resultFile.to_csv(<span class="string">&#x27;data/submission.csv&#x27;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>完整代码及数据请<a href="https://github.com/Ubivision/ML-ALgorithms">点击这里</a></p>
<h2 id="总结">总结</h2>
<p>经过上述过程，最终在训练集上准确率为79.3%，在测试集上准确率为76.8%。可以改进之处我觉得有两个方面，一方面是对于特征的分析和选择，另一方面就是方法，使用专门的分类器算法势必会让准确率有所提升。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>预测问题</tag>
        <tag>分类问题</tag>
        <tag>监督学习</tag>
        <tag>判别模型</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习总结——感知机</title>
    <url>/2021/07/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E2%80%94%E2%80%94%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    <content><![CDATA[<h2 id="关键字">关键字</h2>
<p>二分类；线性模型；判别模型；分离超平面；基于误分类的Loss；随机梯度下降<span id="more"></span></p>
<h2 id="数据集的线性可分性">数据集的线性可分性</h2>
<p>给定一个只有正负两类的数据集<span class="math inline">\(T\)</span>，如果某个超平面<span class="math inline">\(wx+b=0\)</span>能够将正负两类的实例点完全正确地划分在超平面的两侧，即对于所有正实例，有<span class="math inline">\(wx_i+b&gt;0\)</span>，对所有负实例，有<span class="math inline">\(wx_i+b&lt;0\)</span>​，则称<span class="math inline">\(T\)</span>为线性可分数据集，否则线性不可分。</p>
<p>只有满足线性可分的数据集，才能使用感知机算法，否则感知机算法将无法收敛。</p>
<h2 id="感知机学习策略">感知机学习策略</h2>
<p>对于误分类的数据(<span class="math inline">\(x_i\)</span>, <span class="math inline">\(y_i\)</span>​​)来说，标签<span class="math inline">\(y_i\)</span>和预测结果<span class="math inline">\(wx_i+b\)</span>​​肯定是异号的，满足： <span class="math display">\[
-y_i(wx_i+b)&gt;0
\]</span> 所以感知机的损失函数定义为： <span class="math display">\[
L(w,b)=-\sum_{x_i\in M}y_i(wx_i+b)
\]</span> 其中M为误分类点集合，误分类点距离超平面越远，损失函数越大。损失函数对<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>的偏导分别为： <span class="math display">\[
\begin{gathered}
\frac{dL}{dw}=-\sum_{x_i\in M}y_ix_i\\
\frac{dL}{db}=-\sum_{x_i\in M}y_i
\end{gathered}
\]</span></p>
<h2 id="感知机算法原始形式">感知机算法原始形式</h2>
<p>输入：训练数据集<span class="math inline">\(T=\)</span>​​{(<span class="math inline">\(x_i, y_i\)</span>​​)}，<span class="math inline">\(i=1,2,...,n\)</span>​；<span class="math inline">\(y_i\in\)</span>​​​​{-1, +1}​，学习率<span class="math inline">\(\eta(0&lt;\eta \leq 1)\)</span>​</p>
<p>输出：<span class="math inline">\(w,b\)</span>​；感知机模型<span class="math inline">\(f(x)=sign(wx+b)\)</span></p>
<p>步骤：</p>
<p>1.选取初值<span class="math inline">\(w_0,b_0\)</span>；</p>
<p>2.在训练集中选取数据(<span class="math inline">\(x_i,y_i\)</span>)；</p>
<p>3.如果<span class="math inline">\(y_i(wx_i+b)\leq 0\)</span>， <span class="math display">\[
\begin{gathered}
w=w+\eta y_ix_i\\
b=b+\eta y_i
\end{gathered}
\]</span> 则转至第二步，直到训练集中没有误分类点为止。</p>
<h2 id="感知机算法的对偶形式">感知机算法的对偶形式</h2>
<p>如果<span class="math inline">\(w,b\)</span>​的初值都为0，通过原始形式的更新方法，设修改<span class="math inline">\(n\)</span>​次<span class="math inline">\(w,b\)</span>​，则<span class="math inline">\(w,b\)</span>​的增量分别是<span class="math inline">\(\alpha _iy_ix_i\)</span>​和<span class="math inline">\(\alpha _i y_i\)</span>​，这里的<span class="math inline">\(\alpha _i=n_i\eta\)</span>​，所以最后学习到的<span class="math inline">\(w,b\)</span>​可以分别表示为： <span class="math display">\[
\begin{gathered}
w=\sum_{i=1}^{N}\alpha _iy_yx_i\\
b=\sum_{i=1}^{N}\alpha _iy_i
\end{gathered}
\]</span> 输入：训练数据集<span class="math inline">\(T=\)</span>{(<span class="math inline">\(x_i, y_i\)</span>)}，<span class="math inline">\(i=1,2,...,n\)</span>；<span class="math inline">\(y_i\in\)</span>{-1, +1}，学习率<span class="math inline">\(\eta(0&lt;\eta \leq 1)\)</span></p>
<p>输出：<span class="math inline">\(\alpha ,b\)</span>​；感知机模型<span class="math inline">\(f(x)=sign(\sum_{j=1}^{N}\alpha y_jx_jx+b)\)</span></p>
<p>步骤：</p>
<p>1.<span class="math inline">\(\alpha=0,b=0\)</span>​；</p>
<p>2.在训练集中选取数据(<span class="math inline">\(x_i,y_i\)</span>)；</p>
<p>3.如果<span class="math inline">\(y_i(\sum_{j=1}^{N}\alpha y_jx_jx_i+b)\leq 0\)</span>​, <span class="math display">\[
\begin{gathered}
\alpha _i = \alpha _i+\eta\\
b = b+\eta y_i
\end{gathered}
\]</span> 则转至第二步，直到训练集中没有误分类点为止。</p>
<p>第三步中<span class="math inline">\(\alpha _i\)</span>的更新，理解如下： <span class="math display">\[
\begin{gathered}
\alpha _i = \alpha _i+\eta\\
代入\alpha _i=n_i\eta：\\
\alpha = n_i\eta + \eta=\eta (n_i+1)
\end{gathered}
\]</span> 就相当于某点被误分类的次数+1。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类问题</tag>
        <tag>监督学习</tag>
        <tag>判别模型</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实践——逻辑回归</title>
    <url>/2021/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h2 id="概述">概述</h2>
<p>本篇主要的目的是：从实践角度去理解逻辑回归的过程（其中的公式、表达式均以向量化的形式出现），数据集依旧使用Titanic。<span id="more"></span></p>
<h2 id="逻辑回归与线性回归的联系">逻辑回归与线性回归的联系</h2>
<p>逻辑回归模型，其实就是在线性回归的基础上，套了一个Sigmoid函数，换句话说：逻辑回归模型的表达式是一个两层复合函数，其中内层是线性回归模型，外层是Sigmoid函数。通过这种做法，使得线性回归的输出被压缩在0到1之间，以便完成二分类任务。</p>
<p><span class="math display">\[
\begin{aligned}
Sigmoid函数：H &amp;=\frac{1}{1+e^{-Z}} \\
线性模型：Z &amp;=X W
\end{aligned}
\]</span> 其中的<span class="math inline">\(X\)</span>​​​的维度是(m, n+1)(n代表特征个数，m代表数据个数，下同)，<span class="math inline">\(W\)</span>​​​​​​​​的维度是(n+1, 1)，n+1中“1”的含义是：偏置b。所以<span class="math inline">\(z\)</span>​​​的维度是(m, 1)，<span class="math inline">\(H\)</span>​​​不涉及矩阵间运算，故维度也是(m, 1)</p>
<h2 id="逻辑回归代价函数的理解">逻辑回归代价函数的理解</h2>
<p>不论是什么模型，我们都希望代价函数能够做到：在模型预测正确的时候代价函数值为0，预测错误的时候代价函数值为错误值和真实值的差距。但是在逻辑回归中，因为解决的是二分类问题，是离散的，结果只有正反两种，如果继续采用MSE作为代价函数，那么在预测错误时所反应出的“有多错”，就不够严重，换句话说，MSE在二分类问题中对待预测错误时的惩罚力度不够。所以这时候就需要另一种代价函数，对待预测错误时能够狠狠地进行惩罚，预测结果越接近真实值，惩罚力度就越小，所以在逻辑回归中，代价函数是以对数函数的形式出现的，可以分为两种情况讨论。</p>
<p>当真实值为1时，此时代价函数为： <span class="math display">\[
Cost=-ln(H)
\]</span> 图像为：</p>
<img src="/2021/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E7%9C%9F%E5%AE%9E%E5%80%BC%E4%B8%BA1.jpg" class>
<p>当真实值为0时，此时代价函数为： <span class="math display">\[
Cost=-ln(1-H)
\]</span> 图像为：</p>
<img src="/2021/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E2%80%94%E2%80%94%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E7%9C%9F%E5%AE%9E%E5%80%BC%E4%B8%BA0.jpg" class>
<p>将两种情况结合起来考虑，就得到了逻辑回归的代价函数： <span class="math display">\[
Cost=-Yln(H)-(1-Y)ln(1-H)
\]</span> 当然，定义中的代价函数还有一个常数系数1/m，因为上述内容是在阐述原理，所以没有加。</p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">x, y, theta</span>):</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    z = x.dot(theta)</span><br><span class="line">    h = <span class="number">1</span> / (np.exp(-z) + <span class="number">1</span>)</span><br><span class="line">    cost = -<span class="number">1</span>/m*np.<span class="built_in">sum</span>((y*np.log(h)+(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-h)))</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<h2 id="梯度下降">梯度下降</h2>
<h3 id="求偏导">1.求偏导</h3>
<p>思路还是那个思路，对代价函数中的参数求偏导得到梯度，然后参数进行反梯度方向的更新。故首先要做的就是对代价函数求导。观察代价函数可以发现，如果要对参数<span class="math inline">\(w\)</span>​​进行求导，其实就是进行复合函数求导： <span class="math display">\[
\begin{gathered}
\frac{d C o s t}{d W}=\frac{d C}{d H} * \frac{d H}{d Z} * \frac{d Z}{d W} \\
=X^{T} *\left\{\left(-\frac{Y}{H}-\frac{1-Y}{1-H} * -1\right) * H(1-H)\right\} \\
=X^{T} *\left\{\left(\frac{\left(-Y(1-H)+H\left(1-Y\right)\right)}{H(1-H)}\right) * H(1-H)\right\} \\
=X^{T}\left\{\left(-Y+Y H+H-H Y\right)\right\} \\
=X^{T}\left(H-Y\right)
\end{gathered}
\]</span> 上述运算是直接基于矩阵的运算，乍一看可能非常劝退，但是细看其实并不复杂，我们一部分一部分拆开来看： <span class="math display">\[
\begin{gathered}
Cost=-Yln(H)-(1-Y)ln(1-H)\\
\frac{d C}{d H}=(-\frac{Y}{H}-\frac{1-Y}{1-H} *-1)
\end{gathered}
\]</span> 这部分是代价函数对Sigomid函数<span class="math inline">\(H\)</span>求导，涉及到<span class="math inline">\(lnx\)</span>​​​的导数和复合函数求导法则。 <span class="math display">\[
\begin{gathered}
H=\frac{1}{1+e^{-Z}} \\
\frac{d H}{d Z}=\frac{-e^{-Z}*-1}{\left(1+e^{-Z}\right)^{2}}=\frac{1+e^{-Z}-1}{\left(1+e^{-Z}\right)^{2}}=\frac{1}{1+e^{-Z}}\left(1-\frac{1}{1+e^{-Z}}\right)=H(1-H)
\end{gathered}
\]</span> 这部分是Sigmoid函数对线性回归模型<span class="math inline">\(Z\)</span>求导，涉及到分式求导法则和<span class="math inline">\(e^{-x}\)</span>​​​的导数。 <span class="math display">\[
\begin{gathered}
Z=XW\\
\frac{d Z}{d W}=X
\end{gathered}
\]</span> 这部分是线性回归模型<span class="math inline">\(Z\)</span>对参数<span class="math inline">\(W\)</span>的导数。</p>
<p>在这些都清楚了之后，剩下的步骤就是运算了，没有什么理解上的难度。</p>
<p>这时候我们再回过头看化简过后的结果： <span class="math display">\[
\begin{gathered}
\frac{d C o s t}{d W}=X^{T}(H-Y) \\
代入H：\\
\frac{d C o s t}{d W}=X^{T}\left(\frac{1}{1+e^{-X W}}-Y\right)
\end{gathered}
\]</span> 可能在这里会有疑问，上面才解释过<span class="math inline">\(Z\)</span>​对参数<span class="math inline">\(W\)</span>​的导数是<span class="math inline">\(X\)</span>​，这里怎么变成了<span class="math inline">\(X^{T}\)</span>​？而且本来应该右乘，这里为什么变成了左乘？这里先分析一下<span class="math inline">\(H-Y\)</span>​的维度，<span class="math inline">\(H\)</span>​由上述内容内容可知，<span class="math inline">\(H\)</span>​的维度是(m, 1)，<span class="math inline">\(Y\)</span>​的维度也是(m, 1)，所以<span class="math inline">\(H-Y\)</span>​的维度自然也是(m, 1)。<span class="math inline">\(X\)</span>​的维度是(m, n+1)，如果这时候直接右乘<span class="math inline">\(X\)</span>​，是无法相乘的，所以需要转置且变为左乘，这时候<span class="math inline">\(\frac{d C o s t}{d W}\)</span>​​​​​的维度是(n+1, 1)，n+1也正好是参数的个数。</p>
<h3 id="参数更新">2. 参数更新</h3>
<p><span class="math display">\[
W=W-alpha*\frac{d C o s t}{d W}
\]</span></p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span>(<span class="params">x, y, theta, alpha, num_iters</span>):</span></span><br><span class="line">    m = x.shape[<span class="number">0</span>]</span><br><span class="line">    cost_history = np.zeros(num_iters)</span><br><span class="line">    z = x.dot(theta)</span><br><span class="line">    h = <span class="number">1</span> / (np.exp(-z) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        theta = theta - alpha/m*np.dot(x.T, (h-y))</span><br><span class="line">        cost_history[i] = cost(x, y, theta)</span><br><span class="line">    <span class="keyword">return</span> theta, cost_history</span><br></pre></td></tr></table></figure>
<p>完整代码及数据请<a href="https://github.com/Ubivision/ML-ALgorithms">点击这里</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类问题</tag>
        <tag>监督学习</tag>
        <tag>判别模型</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习总结——朴素贝叶斯</title>
    <url>/2021/08/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E2%80%94%E2%80%94%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
    <content><![CDATA[<h2 id="概述">概述</h2>
<p>朴素贝叶斯是一种基于<strong>贝叶斯定理</strong>和<strong>特征条件独立性假设</strong>的分类方法<span id="more"></span>，它的主要思路是：根据贝叶斯定理 <span class="math display">\[
P(A \mid B)=\frac{P(A) P(B \mid A)}{P(B)}
\]</span> 来计算给定输入样例<span class="math inline">\(x\)</span>​的情况下，该样例属于的类别<span class="math inline">\(y\)</span>​，将后验概率最大的类作为<span class="math inline">\(x\)</span>的输出。修改一下上述公式，也就是 <span class="math display">\[
P(属于某一类别y \mid 输入样例x)=\frac{P(属于某一类别y) P(输入样例x \mid 属于某一类别y)}{P(输入样例x)}
\]</span> 此时，问题就转换为了求解<span class="math inline">\(P(属于某一类别y)、P(输入样例x \mid 属于某一类别y)\)</span>​以及<span class="math inline">\(P(输入样例x)\)</span>​​​​，接下来我们分别研究上述三个概率该如何求解。</p>
<h2 id="朴素贝叶斯分类器">朴素贝叶斯分类器</h2>
<p>在开始推导公式之前，让我们先把上述三个概率的文字描述转换为数学描述： <span class="math display">\[
\begin{gathered}
P(属于某一类别y)\iff P(Y=c_k)\\
P(输入样例x)\iff P(X=x)\\
P(输入样例x \mid 属于某一类别y)\iff P(X=x\mid Y=c_k)
\end{gathered}
\]</span> 其中，<span class="math inline">\(c_k\)</span>​代指输出<span class="math inline">\(Y\)</span>​的某种可能，角标<span class="math inline">\(k\)</span>​假设有<span class="math inline">\(k\)</span>​种可能；<span class="math inline">\(x\)</span>​代指输入样例的特征向量，这里可能会产生疑问：刚才还是输入样例<span class="math inline">\(x\)</span>​呢，现在怎么变成样例的特征向量了呢？如果这个输入样例只有一个特征，那继续把<span class="math inline">\(x\)</span>​说成输入样例也勉强可以，但是更多的情况是，输入样例有很多种特征，所以<span class="math inline">\(x\)</span>​代指输入样例的特征向量。这时候<span class="math inline">\(P(X=x\mid Y=c_k)\)</span>​展开写，就是： <span class="math display">\[
\begin{aligned}
P\left(X=x \mid Y=c_{k}\right) &amp;=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right) \\
\end{aligned}
\]</span> 我们会发现，如果将输入样例的每种特征的每种可能都穷举出来，并排列组合计算上述条件概率，会是一件非常复杂且难以计算的工作。因为在现实情况中，每种特征之间会互相影响，比如两种特征中同时存在“下雨”和“低温”，而下雨通常伴随着低温，这就给条件概率的计算又增加了难度，所以，我们需要引入一种假设，来降低这种计算难度，这就是条件独立性假设：</p>
<p><span class="math display">\[
\begin{aligned}
P\left(X=x \mid Y=c_{k}\right) &amp;=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right) \\
&amp;=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
\end{aligned}
\]</span> 也就是说，我们将无视各种特征之间可能存在的联系，以换取计算的方便，代价就是会牺牲一定的分类准确率。</p>
<p>我们再来分析<span class="math inline">\(P(X=x)\)</span>​​，​这部分我想通过已经给出的公式来倒推其中所表达的意思，首先先看给出的公式： <span class="math display">\[
P(X=x)\iff \sum_{k} P\left(X=x \mid Y=c_{k}\right) P\left(Y=c_{k}\right)
\]</span> 单看求和符号右边的式子，我们对其运用条件概率公式 <span class="math display">\[
P(A \mid B)=\frac{P(AB)}{P(B)}
\]</span> 就可以将其转换为： <span class="math display">\[
\sum_{k}P(X=x,Y=c_k)
\]</span> 也就是说，将一个条件概率与<span class="math inline">\(P\left(Y=c_{k}\right)\)</span>的乘积，转换为了一个联合概率，我们再带上求和符合完整的观察这个联合概率，这个式子的意思，就是在所有类别中，特征向量<span class="math inline">\(x\)</span>出现的概率，那其实就是特征向量<span class="math inline">\(x\)</span>出现的概率，也就是<span class="math inline">\(P(X=x)\)</span>​，这也就解释了公式中二者为什么等价。</p>
<p>对于<span class="math inline">\(P\left(Y=c_{k}\right)\)</span>​，可以直接通过数据集进行计算，不需要变形。</p>
<p>在分别分析了这三部分之后，我们可以带入原式进行整体的分析了： <span class="math display">\[
\begin{aligned}
P\left(Y=c_{k} \mid X =x\right)&amp;=\frac{P\left(X=x \mid Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x \mid Y=c_{k}\right) P\left(Y=c_{k}\right)}\\
&amp;=\frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}
\end{aligned}
\]</span> 我们在最开始的时候说到，朴素贝叶斯法最后会将后验概率最大的类作为<span class="math inline">\(x\)</span>​​的输出，也就是说我们就要求让这个式子的值最大化时候<span class="math inline">\(c_{k}\)</span>​​的取值​，所以朴素贝叶斯分类器可以表示为： <span class="math display">\[
y=f(x)=\arg \max _{c_{k}} \frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}
\]</span> 注意到，对于上式，分母对于所有<span class="math inline">\(c_{k}\)</span>​来说都是一样的，所以也可以不看分母，只看分子： <span class="math display">\[
y=f(x)=\arg \max _{c_{k}}P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
\]</span> 这就得到了朴素贝叶斯分类器的表达式。</p>
<h2 id="极大似然估计">极大似然估计</h2>
<p>在朴素贝叶斯法中，可以应用极大似然估计法估计相应的概率。先验概率<span class="math inline">\(P(Y=c_{k})\)</span>​的极大似然估计是：</p>
<p><span class="math display">\[
\begin{equation}
 P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K 
\end{equation}
\]</span> 其中的<span class="math inline">\(I\)</span>​是指示函数，分子所代表的意思，就是统计数据集中有多少个实例的类别是<span class="math inline">\(c_{k}\)</span>​，听起来很复杂，其实很简单。比如现在有个包含20个实例的数据集，其中分为2类，分别是0和1，都各包含10个实例。所以此时的<span class="math inline">\(c_{k}\)</span>​就只有<span class="math inline">\(c_{1}\)</span>​和<span class="math inline">\(c_{2}\)</span>​，取值分别是0和1，<span class="math inline">\(P(Y=c_{1})\)</span>​和<span class="math inline">\(P(Y=c_{2})\)</span>​​​都是10/20，也就是1/2。</p>
<p>设第<span class="math inline">\(j\)</span>个特征<span class="math inline">\(x^{(j)}\)</span>可能取值的集合为{<span class="math inline">\(a_{j1},a_{j2},...,a_{jl}\)</span>}，其中<span class="math inline">\(l=1,2,...,s_{j}\)</span>，条件概率<span class="math inline">\(P(X^{(j)}=a_{jl} \mid Y=c_{k})\)</span>的极大似然估计是： <span class="math display">\[
P\left(X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}
\]</span> 虽然分子分母看起来非常复杂，其实也非常好理解，这个条件概率的意思是：在属于某一类别的前提下，实例中某一特征的某一取值出现的概率。所以分母所代表的，是某一类别的实例的个数，分子代表的，是在属于这一类别的实例的前提下，某一特征取到某一值的个数。</p>
<p><del>这里我有个疑问，我感觉上述计算概率的方式，是在用频率替代概率，也就是运用大数定理来计算的，为什么说是运用极大似然估计的方式来计算的呢？</del></p>
<p>经过询问同学得知，用频率替代概率就是对于离散值的极大似然估计，证明如下：</p>
<p>假设<span class="math inline">\(x=x_{i}\)</span>出现的次数为<span class="math inline">\(\alpha\)</span>，样本总数为<span class="math inline">\(N\)</span>，则似然函数<span class="math inline">\(L(\theta \mid x)=p^{\alpha}(1-p)^{N-\alpha}\)</span>。最大化似然函数，即求得使<span class="math inline">\(L(\theta \mid x)\)</span>最大时的<span class="math inline">\(p\)</span>： <span class="math display">\[
\begin{aligned}
\frac{\partial L(\theta \mid x)}{\partial p} &amp;=\alpha p^{\alpha -1}(1-p)^{N-\alpha }-(N-\alpha )p^{\alpha }(1-p)^{N-\alpha -1}\\
&amp;=p^{\alpha -1}(1-p)^{N-\alpha -1}[\alpha (1-p)-(N-\alpha )p]\\
&amp;=p^{\alpha -1}(1-p)^{N-\alpha -1}[\alpha -\alpha p-Np+\alpha p]\\
&amp;=p^{\alpha -1}(1-p)^{N-\alpha -1}(\alpha -Np)=0
\end{aligned}
\]</span> 当<span class="math inline">\(p=\frac{\alpha }{N}\)</span>时，<span class="math inline">\(L(\theta \mid x)\)</span>​取得最大值，即用频率替代概率。</p>
<h2 id="贝叶斯估计">贝叶斯估计</h2>
<p>用极大似然估计可能会出现所要估计的概率值为0的情况，比如某一特征的某一取值在当前数据集中不存在，那么概率就会是0，这在带入朴素贝叶斯分类器之后会导致整个式子都是0，无法计算，也就是说影响到了后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。具体来说，条件概率的贝叶斯估计是： <span class="math display">\[
\begin{equation}
P_{\lambda}\left(X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+S_{j} \lambda}
\end{equation}
\]</span> 式中<span class="math inline">\(\lambda \geq\)</span> 0。等价于在随机变量各个取值的频数上赋予一个正数<span class="math inline">\(\lambda\)</span>&gt;0。当<span class="math inline">\(\lambda\)</span>​=0时就是极大似然估计。常取 λ= 1，这时称为<strong>拉普拉斯平滑（Laplacian smoothing）</strong>。</p>
<p>问题在于，为什么分子加了一个<span class="math inline">\(\lambda\)</span>，而分母加了一个<span class="math inline">\(S_{j} \lambda\)</span>​呢？我们先来看运用极大似然估计时必定满足的一个等式： <span class="math display">\[
\frac{\sum_{l=1}^{s_j}\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}=\sum_{l=1}^{s_j}P\left(X^{(j)}=a_{j l} \mid Y=c_{k}\right)=1
\]</span> 上式的含义是：在属于某一类别的前提下，某一特征取全部值时出现的概率。分析一下这句话，当某一特征的取值全部被取到的时候，是必定包含全部实例的，所以不管条件概率的条件是什么，上式的值都是1。如果我们想给分子分母都添加一项，但要让该分式的值依然为1，那么自然想到给分子分母都加上一个相同的项即可，这里我们给分子分母都加上<span class="math inline">\(S_{j} \lambda\)</span>​，​就变成了： <span class="math display">\[
\frac{\sum_{l=1}^{s_j}\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)+S_{j} \lambda}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+S_{j} \lambda}
\]</span> 之所以要加上<span class="math inline">\(S_{j} \lambda\)</span>，是因为当<span class="math inline">\(\sum_{l=1}^{s_j}\)</span>展开时，会出现<span class="math inline">\(S_{j}\)</span>​​项，所以单看每一项时，就得到： <span class="math display">\[
\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+S_{j} \lambda}
\]</span> 这也就是条件概率的贝叶斯估计。</p>
<p>同理，先验概率的贝叶斯估计是： <span class="math display">\[
P_{\lambda}\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+\lambda}{N+K \lambda}
\]</span></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类问题</tag>
        <tag>监督学习</tag>
        <tag>生成模型</tag>
      </tags>
  </entry>
</search>
