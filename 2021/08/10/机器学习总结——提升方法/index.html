<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.6.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<meta name="description" content="概述 提升方法（boosting）的基本思路是：将弱可学习算法提升为强可学习算法。在分类问题中，它通过每一轮改变训练样本的权重，学习多个分类器，并将所有分类器进行线性组合，得到一个性能更好的分类器。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习总结——提升方法">
<meta property="og:url" content="http://example.com/2021/08/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E2%80%94%E2%80%94%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/index.html">
<meta property="og:site_name" content="Messing Arounder">
<meta property="og:description" content="概述 提升方法（boosting）的基本思路是：将弱可学习算法提升为强可学习算法。在分类问题中，它通过每一轮改变训练样本的权重，学习多个分类器，并将所有分类器进行线性组合，得到一个性能更好的分类器。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-08-10T00:44:11.000Z">
<meta property="article:modified_time" content="2021-08-11T12:05:31.816Z">
<meta property="article:author" content="Rex">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="优化方法">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2021/08/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E2%80%94%E2%80%94%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2021/08/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E2%80%94%E2%80%94%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/","path":"2021/08/10/机器学习总结——提升方法/","title":"机器学习总结——提升方法"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习总结——提升方法 | Messing Arounder</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Messing Arounder</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">——学习小屋</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adaboost%E7%AE%97%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">AdaBoost算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF"><span class="nav-number">2.1.</span> <span class="nav-text">1.基本思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="nav-number">2.2.</span> <span class="nav-text">2.算法步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adaboost%E4%B8%A4%E5%A4%A7%E7%89%B9%E7%82%B9"><span class="nav-number">2.3.</span> <span class="nav-text">3.AdaBoost两大特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adaboost%E7%AE%97%E6%B3%95%E7%9A%84%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90"><span class="nav-number">2.4.</span> <span class="nav-text">4.AdaBoost算法的误差分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E7%90%861adaboost%E7%AE%97%E6%B3%95%E6%9C%80%E7%BB%88%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E7%95%8C%E4%B8%BA"><span class="nav-number">2.4.1.</span> <span class="nav-text">定理1：AdaBoost算法最终分类器的训练误差界为</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E7%90%862%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98adaboost%E7%9A%84%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E8%BE%B9%E7%95%8C%E4%B8%BA"><span class="nav-number">2.4.2.</span> <span class="nav-text">定理2：二分类问题AdaBoost的训练误差边界为</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adaboost%E7%AE%97%E6%B3%95%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E8%A7%A3%E9%87%8A"><span class="nav-number">2.5.</span> <span class="nav-text">5.AdaBoost算法的另一种解释</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E7%AE%97%E6%B3%95"><span class="nav-number">2.5.1.</span> <span class="nav-text">前向分步算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E7%AE%97%E6%B3%95%E4%B8%8Eadaboost"><span class="nav-number">2.5.2.</span> <span class="nav-text">前向分步算法与AdaBoost</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8F%90%E5%8D%87%E6%A0%91"><span class="nav-number">3.</span> <span class="nav-text">提升树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0-1"><span class="nav-number">3.1.</span> <span class="nav-text">1.概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E7%9A%84%E6%8F%90%E5%8D%87%E6%A0%91%E7%AE%97%E6%B3%95"><span class="nav-number">3.2.</span> <span class="nav-text">2.回归问题的提升树算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87"><span class="nav-number">3.3.</span> <span class="nav-text">3.梯度提升</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Rex</p>
  <div class="site-description" itemprop="description">记录经验</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/Ubivision" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E2%80%94%E2%80%94%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Rex">
      <meta itemprop="description" content="记录经验">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Messing Arounder">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习总结——提升方法
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-08-10 08:44:11" itemprop="dateCreated datePublished" datetime="2021-08-10T08:44:11+08:00">2021-08-10</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-08-11 20:05:31" itemprop="dateModified" datetime="2021-08-11T20:05:31+08:00">2021-08-11</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>13k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>12 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="概述">概述</h2>
<p>提升方法（boosting）的基本思路是：将弱可学习算法提升为强可学习算法。在分类问题中，它通过每一轮改变训练样本的权重，学习多个分类器，并将所有分类器进行线性组合，得到一个性能更好的分类器。<span id="more"></span></p>
<h2 id="adaboost算法">AdaBoost算法</h2>
<h3 id="基本思路">1.基本思路</h3>
<p>通过多次训练得到多个弱分类器，在每一次训练后提高被误分类样本的权重，降低被正确分类样本的权重，这样会使得被误分类的样本在后续轮次中受到更大的关注，同时根据分类结果计算当前分类器的权重。在所有基分类器都训练结束后，将所有基分类器进行线性组合，系数就是每轮训练后得到的分类器权重，从而组合成为一个性能更优的分类器。</p>
<h3 id="算法步骤">2.算法步骤</h3>
<p>输入：训练数据集<span class="math inline">\(T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}\)</span>​，其中<span class="math inline">\(x_i\in X\subseteq R^n\)</span>​，<span class="math inline">\(y_i \in Y={-1,+1}\)</span>​；一种弱学习算法；</p>
<p>输出：最终分类器<span class="math inline">\(G(x)\)</span>。</p>
<p>（1）初始化训练数据的权值分布；（权值相同） <span class="math display">\[
D_{1}=\left(w_{11}, \cdots, w_{1 i}, \cdots, w_{1 N}\right), \quad w_{1 i}=\frac{1}{N}, \quad i=1,2, \cdots, N
\]</span> （2）对<span class="math inline">\(m=1,2,...,M\)</span>​​​（进行M轮训练，每次训练后更新数据集权值，最终得到M个基本分类器）</p>
<p>​ （a）使用具有权值分布<span class="math inline">\(D_M\)</span>​​的训练数据集学习，得到基本分类器<span class="math inline">\(G_m(x)\)</span>​，</p>
<p>​ （b）计算<span class="math inline">\(G_m(x)\)</span>​在训练数据集上的分类误差率， <span class="math display">\[
\begin{aligned}
e_{m}&amp;=\sum_{i=1}^{N} P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)\\
&amp;=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)\\
&amp;=\sum_{G_m(x_i)\neq y_i}w_mi
\end{aligned}
\]</span> 其中<span class="math inline">\(w_mi\)</span>​表示第<span class="math inline">\(m\)</span>​轮中第<span class="math inline">\(i\)</span>​个实例的权值，<span class="math inline">\(\sum_{i=1}^{N}w_mi=1\)</span>​， <span class="math inline">\(\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right)\neq y_{i}\right)\)</span>​是被<span class="math inline">\(G_m(x)\)</span>​误分类样本的权值之和。</p>
<p>​ （c）计算<span class="math inline">\(G_m(x)\)</span>​的系数（基本分类器的权值，权值越大代表该分类器越有“话语权”） <span class="math display">\[
\alpha _m = \frac{1}{2}ln\frac{1-e_M}{e_m}
\]</span> 可以发现，当<span class="math inline">\(e_m\le \frac{1}{2}\)</span>时，系数<span class="math inline">\(\alpha _m\ge 0\)</span>，且<span class="math inline">\(\alpha _m\)</span>随着<span class="math inline">\(e_m\)</span>的减小而增大，所以分类误差越小的基本分类在最终分类器中作用越大。</p>
<p>​ （d）更新训练数据集的权值分布 <span class="math display">\[
\begin{gathered}
D_{m+1}=\left(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right) \\
w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), \quad i=1,2, \cdots, N
\end{gathered}
\]</span> ​ 其中的<span class="math inline">\(Z_m\)</span>是规范化因子： <span class="math display">\[
Z_{m}=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)
\]</span> ​ 而且，式（4）可以写成： <span class="math display">\[
w_{m+1, i}= \begin{cases}\frac{w_{m i}}{Z_{m}} \mathrm{e}^{-\alpha_{m}}, &amp; G_{m}\left(x_{i}\right)=y_{i} \\ \frac{w_{m i}}{Z_{m}} \mathrm{e}^{\alpha_{m}}, &amp; G_{m}\left(x_{i}\right) \neq y_{i}\end{cases}
\]</span> 因为是二分类任务，所以<span class="math inline">\(G_m(x_i)\)</span>和<span class="math inline">\(y_i\)</span>都只会有两种结果：-1和+1，当<span class="math inline">\(G_{m}(x_{i})=y_{i}\)</span>时，二者同号，所以是<span class="math inline">\(-\alpha _m\)</span>，二者不相等时异号，所以是<span class="math inline">\(\alpha _m\)</span>。</p>
<p>（3）将基本分类器进行线性组合 <span class="math display">\[
f(x)=\sum_{m=1}^{M}\alpha _M G_m(x)
\]</span> ​ 得到最终分类器 <span class="math display">\[
\begin{aligned}
G(x) &amp;=sign(f(x))\\
&amp;=sign(\sum_{m=1}^{M}\alpha_m G_m(x))
\end{aligned}
\]</span> AdaBoost算法举例详见统计学习方法P158</p>
<h3 id="adaboost两大特点">3.AdaBoost两大特点</h3>
<p>（1）不改变训练数据，而改变训练数据的权值，使得训练数据在基本分类器的学习中起不同的作用。</p>
<p>（2）最终分类器是基本分类器的线性组合。</p>
<h3 id="adaboost算法的误差分析">4.AdaBoost算法的误差分析</h3>
<h4 id="定理1adaboost算法最终分类器的训练误差界为">定理1：AdaBoost算法最终分类器的训练误差界为</h4>
<p><span class="math display">\[
\frac{1}{N} \sum_{i=1}^{N} I\left(G\left(x_{i}\right) \neq y_{i}\right) \leqslant \frac{1}{N} \sum_{i} \exp \left(-y_{i} f\left(x_{i}\right)\right)=\prod_{m} Z_{m}
\]</span></p>
<p>先证明不等式：</p>
<p>当<span class="math inline">\(G(x_{i}) \neq y_{i}\)</span>时，<span class="math inline">\(I(G(x_i) \neq y_i)\)</span>值为小于等于1的数，此时<span class="math inline">\(y_if(x_i)&lt;0\)</span>，<span class="math inline">\(exp(-y_if(x_i)) \ge 1\)</span>，故<span class="math inline">\(I(G(x_i) \neq y_i) \le exp(-y_if(x_i))\)</span>；</p>
<p>当<span class="math inline">\(G(x_{i})=y_{i}\)</span>时，<span class="math inline">\(I(G(x_i) \neq y_i)\)</span>值为0，此时<span class="math inline">\(y_if(x_i)&gt;0\)</span>，<span class="math inline">\(0&lt;exp(-y_if(x_i))&lt;1\)</span>，故<span class="math inline">\(I(G(x_i) \neq y_i) \le exp(-y_if(x_i))\)</span>​；</p>
<p>所以在任何情况下，式（9）中的不等式都成立。</p>
<p>再证明后半部分的等式： <span class="math display">\[
\frac{1}{N} \sum_{i} \exp \left(-y_{i} f\left(x_{i}\right)\right)
\]</span> <span class="math inline">\(f(x_i)\)</span>就是可以展开，写成<span class="math inline">\(\alpha _mG_m(x_i)\)</span>，故得到： <span class="math display">\[
\frac{1}{N} \sum_{i} \exp \left(-\sum_{m=1}^{M} \alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)
\]</span> 此处的系数<span class="math inline">\(\frac{1}{N}\)</span>，就是第一轮训练数据集的权重，可以改写为<span class="math inline">\(w_1i\)</span>，同时把指数相加改写为整体相乘的形式，得到： <span class="math display">\[
\sum_{i} w_{1 i} \prod_{m=1}^{M} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)
\]</span> 根据上式（5）中<span class="math inline">\(Z_m\)</span>的定义，可以改写为： <span class="math display">\[
Z_{1} \sum_{i} w_{2 i} \prod_{m=2}^{M} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)
\]</span> 再继续代入式（5）的定义，最终可以得到： <span class="math display">\[
\begin{aligned}
&amp;Z_{1} Z_{2} \cdots Z_{M-1} \sum_{i} w_{M i} \exp \left(-\alpha_{M} y_{i} G_{M}\left(x_{i}\right)\right) \\
&amp;=\prod_{m=1}^{M} Z_{m}
\end{aligned}
\]</span> 合并起来写，就是： <span class="math display">\[
\begin{aligned}
\frac{1}{N} \sum_{i} \exp \left(-y_{i} f\left(x_{i}\right)\right) &amp;=\frac{1}{N} \sum_{i} \exp \left(-\sum_{m=1}^{M} \alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right) \\
&amp;=\sum_{i} w_{1 i} \prod_{m=1}^{M} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right) \\
&amp;=Z_{1} \sum_{i} w_{2 i} \prod_{m=2}^{M} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right) \\
&amp;=Z_{1} Z_{2} \sum_{i} w_{3 i} \prod_{m=3}^{M} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right) \\
&amp;=\cdots \\
&amp;=Z_{1} Z_{2} \cdots Z_{M-1} \sum_{i} w_{M i} \exp \left(-\alpha_{M} y_{i} G_{M}\left(x_{i}\right)\right) \\
&amp;=\prod_{m=1}^{M} Z_{m}
\end{aligned}
\]</span> 这一定理说明，通过在每一轮选取适当的基本分类器<span class="math inline">\(G_m\)</span>，可以使得<span class="math inline">\(Z_m\)</span>​最小，从而使训练误差下降最快。</p>
<h4 id="定理2二分类问题adaboost的训练误差边界为">定理2：二分类问题AdaBoost的训练误差边界为</h4>
<p><span class="math display">\[
\begin{aligned}
\prod_{m=1}^{M} Z_{m} &amp;=\prod_{m=1}^{M}\left[2 \sqrt{e_{m}\left(1-e_{m}\right)}\right] \\
&amp;=\prod_{m=1}^{M} \sqrt{\left(1-4 \gamma_{m}^{2}\right)} \\
&amp; \leqslant \exp \left(-2 \sum_{m=1}^{M} \gamma_{m}^{2}\right)
\end{aligned}
\]</span></p>
<p>此处，<span class="math inline">\(\gamma _m = \frac{1}{2}-e_m\)</span></p>
<p>等式部分证明如下：</p>
<p>由<span class="math inline">\(Z_m\)</span>​​​​的定义式（5）可得： <span class="math display">\[
\begin{aligned}
Z_{m} &amp;=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right) \\
&amp;=\sum_{y_{i}=G_{m}\left(x_{i}\right)} w_{m i} \mathrm{e}^{-\alpha_{m}}+\sum_{y_{i} \neq G_{m}\left(x_{i}\right)} w_{m i} \mathrm{e}^{\alpha_{m}}
\end{aligned}
\]</span> 我们知道<span class="math inline">\(\sum_{i=1}^{N}w_mi=1\)</span>，就是说每轮训练数据集的权重之和为1，而训练数据集中只包含被正确分类的和误分类的两种情况，所以可以拆开写成如式（17）的两部分；</p>
<p>再由分类误差率的定义式（2）可得： <span class="math display">\[
\begin{aligned}
&amp;\sum_{y_{i}=G_{m}\left(x_{i}\right)} w_{m i} \mathrm{e}^{-\alpha_{m}}+\sum_{y_{i} \neq G_{m}\left(x_{i}\right)} w_{m i} \mathrm{e}^{\alpha_{m}}\\
&amp;=\left(1-e_{m}\right) \mathrm{e}^{-\alpha_{m}}+e_{m} \mathrm{e}^{\alpha_{m}}
\end{aligned}
\]</span> 错误率是<span class="math inline">\(e_{m}\)</span>，那么正确率自然就是<span class="math inline">\(1-e_{m}\)</span></p>
<p>将基本分类器的权重<span class="math inline">\(\alpha _m\)</span>的定义式（3）代入式（18），得到： <span class="math display">\[
\begin{aligned}
&amp;\left(1-e_{m}\right) \mathrm{e}^{-\alpha_{m}}+e_{m} \mathrm{e}^{\alpha_{m}}\\
&amp;=(1-e_m)(\frac{1-e_m}{e_m})^\frac{-1}{2}+e_m(\frac{1-e_m}{e_m})^\frac{1}{2}\\
&amp;=(1-e_m)^\frac{1}{2}(e_m)^\frac{1}{2}+(e_m)^\frac{1}{2}(1-e_m)^\frac{1}{2}\\
&amp;=2\sqrt{e_m(1-e_m)}
\end{aligned}
\]</span> 再令<span class="math inline">\(\gamma _m=\frac{1}{2}-e_m\)</span>，最终可以得到： <span class="math display">\[
\begin{aligned}
&amp;2\sqrt{e_m(1-e_m)}\\
&amp;=\sqrt{1-4\gamma _m^{2}}
\end{aligned}
\]</span> 将推导过程全部整合在一起： <span class="math display">\[
\begin{aligned}
Z_{m} &amp;=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right) \\
&amp;=\sum_{y_{i}=G_{m}\left(x_{i}\right)} w_{m i} \mathrm{e}^{-\alpha_{m}}+\sum_{y_{i} \neq G_{m}\left(x_{i}\right)} w_{m i} \mathrm{e}^{\alpha_{m}} \\
&amp;=\left(1-e_{m}\right) \mathrm{e}^{-\alpha_{m}}+e_{m} \mathrm{e}^{\alpha_{m}} \\
&amp;=2 \sqrt{e_{m}\left(1-e_{m}\right)} \\
&amp;=\sqrt{1-4 \gamma_{m}^{2}}
\end{aligned}
\]</span> 对于不等式部分 <span class="math display">\[
\prod_{m=1}^{M} \sqrt{\left(1-4 \gamma_{m}^{2}\right)}
 \leqslant \exp \left(-2 \sum_{m=1}^{M} \gamma_{m}^{2}\right)
\]</span> 证明如下：</p>
<p>首先将不等式改写，将指数相加改写为整体相乘： <span class="math display">\[
\prod_{m=1}^{M} \sqrt{\left(1-4 \gamma_{m}^{2}\right)}
 \leqslant \prod_{m=1}^{M}\exp \left(-2 \gamma_{m}^{2}\right)
\]</span> 再由<span class="math inline">\(e^x\)</span>和<span class="math inline">\(\sqrt{1-x}\)</span>在<span class="math inline">\(x=0\)</span>​处的泰勒展开式： <span class="math display">\[
\begin{gathered}
&amp;e^x=1+x+\frac{1}{2!}x^2+\cdots+\frac{1}{n!}x^n+o(x^n)\\
&amp;(1+x)^\alpha =1+\alpha x+\frac{\alpha (\alpha -1)}{2!}x^2+\cdots +\frac{\alpha (\alpha -1)\cdots (\alpha -n-1)}{n!}x^n+o(x^n)
\end{gathered}
\]</span> 可知<span class="math inline">\(\sqrt{\left(1-4 \gamma_{m}^{2}\right)}\leqslant \exp \left(-2 \gamma_{m}^{2}\right)\)</span>​，进而得以证明。</p>
<h3 id="adaboost算法的另一种解释">5.AdaBoost算法的另一种解释</h3>
<h4 id="前向分步算法">前向分步算法</h4>
<p>输入：训练数据集<span class="math inline">\(T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}\)</span>​​；损失函数<span class="math inline">\(L(y，f(x))\)</span>​​；基函数集{<span class="math inline">\(b(x;\gamma)\)</span>​​}；</p>
<p>输出：加法模型<span class="math inline">\(f(x)\)</span>。</p>
<p><strong>算法步骤：</strong></p>
<p>（1）初始化<span class="math inline">\(f_0(x)=0\)</span>；</p>
<p>（2）对<span class="math inline">\(m=1,2,\cdots ,M\)</span>​​：（相当于M次循环）</p>
<p>​ （a）极小化损失函数 <span class="math display">\[
\left(\beta_{m}, \gamma_{m}\right)=\arg \min _{\beta, \gamma} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta b\left(x_{i} ; \gamma\right)\right)
\]</span> 得到参数<span class="math inline">\(\beta_{m}, \gamma_{m}\)</span>。其中<span class="math inline">\(f_{m-1}\left(x_{i}\right)\)</span>表示前<span class="math inline">\(m-1\)</span>轮所得到的基函数的线性组合， <span class="math inline">\(\beta b\left(x_{i} ; \gamma\right)\)</span>表示当前轮的基函数。</p>
<p>​ （b）更新 <span class="math display">\[
f_m(x)=f_{m-1}(x)+\beta_{m}b(x;\gamma _m)
\]</span> （3）得到加法模型 <span class="math display">\[
f(x)=f_{M}(x)=\sum_{m=1}^{M} \beta_{m} b\left(x ; \gamma_{m}\right)
\]</span> 由上可知，前向分步算法将同时求解从<span class="math inline">\(m=1\)</span>到<span class="math inline">\(M\)</span>中的所有基函数的参数<span class="math inline">\(\beta_{m}, \gamma_{m}\)</span>的优化问题，简化为逐次求解<span class="math inline">\(\beta_{m}, \gamma_{m}\)</span>​​​的优化问题，降低了求解的难度。</p>
<p><strong>前向分步算法的核心思想是：</strong>因为学习的是加法模型，那么优化整体就可以转换成优化每一步，具体做法是每一步只学习一个基函数及其系数，那么到最后得到的加法模型就是最优的。</p>
<h4 id="前向分步算法与adaboost">前向分步算法与AdaBoost</h4>
<p><strong>定理：</strong>AdaBoost算法是前向分步加法算法的特例。模型是由基本分类器组成的加法模型，损失函数是指数函数。</p>
<p>证明如下：</p>
<p>前向分步算法学习的是加法模型，当基函数为基本分类器时，该加法模型等价于AdaBoost的最终分类器 <span class="math display">\[
f(x)=\sum_{m=1}^{M}\alpha _mG_m(x)
\]</span> 因为前向分步算法逐一学习基函数，这一过程与AdaBoost算法逐一学习基本分类器的过程一致。</p>
<p>下面证明前向分步算法的损失函数是指数损失函数<span class="math inline">\(L(y,f(x))=exp[-yf(x)]\)</span>​时，其学习步骤等价于AdaBoost算法的学习步骤：</p>
<p>假设经过<span class="math inline">\(m-1\)</span>轮迭代，前向分步算法已经得到了<span class="math inline">\(f_{m-1}(x)\)</span>，在第<span class="math inline">\(m\)</span>轮迭代得到<span class="math inline">\(\alpha _m\)</span>，<span class="math inline">\(G_m(x)\)</span>，此时<span class="math inline">\(f_m(x)\)</span>可以表达为： <span class="math display">\[
f_m(x)=f_{m-1}(x)+\alpha _m G_m(x)
\]</span> 目标是使前向分步算法得到的<span class="math inline">\(\alpha _m\)</span>和<span class="math inline">\(G_m(x)\)</span>使<span class="math inline">\(f_m(x)\)</span>在训练数据集<span class="math inline">\(T\)</span>上的指数损失最小，即 <span class="math display">\[
\left(\alpha_{m}, G_{m}(x)\right)=\arg \min _{\alpha, G} \sum_{i=1}^{N} \exp \left[-y_{i}\left(f_{m-1}\left(x_{i}\right)+\alpha G\left(x_{i}\right)\right)\right]
\]</span> 式（30）可以表示为 <span class="math display">\[
\left(\alpha_{m}, G_{m}(x)\right)=\arg \min _{\alpha, G} \sum_{i=1}^{N} \bar{w}_{m i} \exp \left[-y_{i} \alpha G\left(x_{i}\right)\right]
\]</span> 其中，<span class="math inline">\(\bar{w}_{m i}=exp[-y_if_{m-1}(x_i)]\)</span>。</p>
<p>现在证明使得式（31）达到最小的<span class="math inline">\(\alpha _m^*\)</span>和<span class="math inline">\(G_m^*(x)\)</span>就是AdaBoost算法所得到的<span class="math inline">\(\alpha _m\)</span>和<span class="math inline">\(G_m(x)\)</span>，求解式（31）：</p>
<p><strong>求<span class="math inline">\(G_m^*(x)\)</span>：</strong></p>
<p>对任意<span class="math inline">\(\alpha &gt;0\)</span>，式（31）可转换为 <span class="math display">\[
G_{m}^{*}(x)=\arg \min _{G} \sum_{i=1}^{N} \bar{w}_{m i} I\left(y_{i} \neq G\left(x_{i}\right)\right)
\]</span> 其中，<span class="math inline">\(\bar{w}_{m i}=exp[-y_if_{m-1}(x_i)]\)</span>​​。式（32）与式（31）的唯一区别在于，将指数损失函数替换成了指示函数，也就是0/1损失函数，至于为什么可以这么替换，西瓜书P174页给出了证明，简言之，就是说当指数损失函数最小化时，分类错误率也将最小化。这说明指数损失函数是分类任务原本0/1损失函数的替代损失函数。</p>
<p>此时得到的分类器<span class="math inline">\(G_{m}^{*}(x)\)</span>即为AdaBoost算法的基本分类器<span class="math inline">\(G_{m}(x)\)</span>，因为它是使第<span class="math inline">\(m\)</span>​轮加权训练数据分类误差率最小的基本分类器。</p>
<p><strong>求<span class="math inline">\(\alpha _m^*\)</span>​：</strong></p>
<p>式（31）中，当基本分类器正确分类实例时，<span class="math inline">\(y_i\)</span>与<span class="math inline">\(G(x_i)\)</span>同号，否则为异号，所以可以改写为： <span class="math display">\[
\sum_{i=1}^{N} \bar{w}_{m i} \exp \left[-y_{i} \alpha G\left(x_{i}\right)\right] =\sum_{y_{i}=G_{m}\left(x_{i}\right)} \bar{w}_{m i} \mathrm{e}^{-\alpha}+\sum_{y_{i} \neq G_{m}\left(x_{i}\right)} \bar{w}_{m i} \mathrm{e}^{\alpha}
\]</span> 其中每一项还有另一种写法： <span class="math display">\[
\begin{aligned}
&amp;\sum_{y_{i}=G_{m}\left(x_{i}\right)} \bar{w}_{m i} \mathrm{e}^{-\alpha}
=e^{-\alpha}\sum_{i=1}^{N}\bar{w}_{m i}I(y_{i} = G\left(x_{i}\right))\\
&amp;\sum_{y_{i}\neq G_{m}\left(x_{i}\right)} \bar{w}_{m i} \mathrm{e}^{-\alpha}
=e^{-\alpha}\sum_{i=1}^{N}\bar{w}_{m i}I(y_{i} \neq G\left(x_{i}\right))
\end{aligned}
\]</span> 而且，这两项之和是： <span class="math display">\[
e^{-\alpha}\sum_{i=1}^{N}\bar{w}_{m i}I(y_{i} = G\left(x_{i}\right))
+e^{-\alpha}\sum_{i=1}^{N}\bar{w}_{m i}I(y_{i} \neq G\left(x_{i}\right))
={e}^{-\alpha} \sum_{i=1}^{N} \bar{w}_{m i}
\]</span> 这个很好理解，分错的实例和分对的实例之和，就是全部。所以式（33）可以进一步展开： <span class="math display">\[
\begin{aligned}
\sum_{i=1}^{N} \bar{w}_{m i} \exp \left[-y_{i} \alpha G\left(x_{i}\right)\right] &amp;=\sum_{y_{i}=G_{m}\left(x_{i}\right)} \bar{w}_{m i} \mathrm{e}^{-\alpha}+\sum_{y_{i} \neq G_{m}\left(x_{i}\right)} \bar{w}_{m i} \mathrm{e}^{\alpha} \\
&amp;=\left(\mathrm{e}^{\alpha}-\mathrm{e}^{-\alpha}\right) \sum_{i=1}^{N} \bar{w}_{m i} I\left(y_{i} \neq G\left(x_{i}\right)\right)+\mathrm{e}^{-\alpha} \sum_{i=1}^{N} \bar{w}_{m i}
\end{aligned}
\]</span> 将刚才求得的<span class="math inline">\(G_m^*(x)\)</span>​代入式（36），对<span class="math inline">\(\alpha\)</span>​​​​​求导并使导数为0，就得到了使式（31）​最小的<span class="math inline">\(\alpha\)</span>​​： <span class="math display">\[
\alpha ^*_m=\frac{1}{2}ln\frac{1-e_m}{e_m}
\]</span></p>
<p>其中，<span class="math inline">\(e_m\)</span>是分类误差率： <span class="math display">\[
\begin{aligned}
e_{m}=&amp; \frac{\sum_{i=1}^{N} \bar{w}_{m i} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}{\sum_{i=1}^{N} \bar{w}_{m i}} \\
=&amp; \sum_{i=1}^{N} w_{m i} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)
\end{aligned}
\]</span> 我们发现，这里的<span class="math inline">\(\alpha ^*_m\)</span>与AdaBoost算法中得到的<span class="math inline">\(\alpha _m\)</span>完全一致。</p>
<p>最后再来看每轮样本权值的更新，由 <span class="math display">\[
f_m(x)=f_{m-1}(x)+\alpha _mG_m(x)
\]</span> 以及<span class="math inline">\(\bar{w}_{m i}=exp[-y_if_{m-1}(x_i)]\)</span>可得： <span class="math display">\[
\bar{w}_{m+1,i}=\bar{w}_{m,i}exp[-y_i\alpha_mG_m(x)]
\]</span> 证明如下： <span class="math display">\[
\begin{aligned}
\bar{w}_{m+1,i}&amp;=exp[-y_if_m(x_i)]\\
&amp;=exp[-y_i(f_{m-1}(x_i)+\alpha_mG_m(x)))]\\
&amp;=exp[-y_if_{m-1}(x_i)-y_i\alpha_mG_m(x)]\\
&amp;=\bar{w}_{m,i}exp[-y_i\alpha_mG_m(x)]
\end{aligned}
\]</span> 所以，式（40）与AdaBoost算法中的样本权值更新相比，只相差规范化因子，因而等价。</p>
<p>综上内容，可证明当加法模型的基函数是基本分类器，损失函数是指数函数时的前向分步算法，即为AdaBoost算法。</p>
<h2 id="提升树">提升树</h2>
<h3 id="概述-1">1.概述</h3>
<p>以决策树为基函数的提升方法称为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树，针对不同基函数的提升树，主要区别在于使用的损失函数不同，比如回归问题使用平方误差损失函数，分类问题使用指数损失函数，以及使用一般损失函数的一般决策问题。对于二分类问题，提升树算法相当于将AdaBoost算法中的基本分类器限制为二叉分类树即可，可以说此时的提升树算法是AdaBoost算法的特例，这里我们主要讨论回归问题的提升树。</p>
<h3 id="回归问题的提升树算法">2.回归问题的提升树算法</h3>
<p>己知一个训练数据集<span class="math inline">\(T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}\)</span>​​，在决策树中已经讨论了回归树的问题。如果将输入空间<span class="math inline">\(X\)</span>​​划分为<span class="math inline">\(J\)</span>​​个互不相交的区域<span class="math inline">\(R_1,R_2,\cdots ,R_J\)</span>​​，并且在每个区域上确定输出的常量<span class="math inline">\(c_j\)</span>​​​那么树可表示为： <span class="math display">\[
T(x ; \Theta)=\sum_{j=1}^{J} c_{j} I\left(x \in R_{j}\right)
\]</span> 其中，<span class="math inline">\(\Theta=\left\{\left(R_{1}, c_{1}\right),\left(R_{2}, c_{2}\right), \cdots,\left(R_{J}, c_{J}\right)\right\}\)</span>表示树的区域划分和各区域上的常数。<span class="math inline">\(J\)</span>​是回归树的复杂度即叶结点个数。</p>
<p>回归问题提升树使用前向分步算法： <span class="math display">\[
\begin{aligned}
&amp;f_{0}(x)=0 \\
&amp;f_{m}(x)=f_{m-1}(x)+T\left(x ; \Theta_{m}\right), \quad m=1,2, \cdots, M \\
&amp;f_{M}(x)=\sum_{m=1}^{M} T\left(x ; \Theta_{m}\right)
\end{aligned}
\]</span> 在前向分步算法的第<span class="math inline">\(m\)</span>步，给定当前模型<span class="math inline">\(f_{m-1}(x)\)</span>​，需求解 <span class="math display">\[
\hat{\Theta}_{m}=\arg \min _{\Theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \Theta_{m}\right)\right)
\]</span> 得到<span class="math inline">\(\hat{\Theta}_{m}\)</span>，即第<span class="math inline">\(m\)</span>棵树的参数。</p>
<p>当采用平方误差损失函数时， <span class="math display">\[
L(y,f(x))=(y-f(x))^2
\]</span> 其损失变为 <span class="math display">\[
\begin{aligned}
L\left(y, f_{m-1}(x)+T\left(x ; \Theta_{m}\right)\right) &amp;=\left[y-f_{m-1}(x)-T\left(x ; \Theta_{m}\right)\right]^{2} \\
&amp;=\left[r-T\left(x ; \Theta_{m}\right)\right]^{2}
\end{aligned}
\]</span> 其中<span class="math inline">\(r=y-f_{m-1}(x)\)</span>，被称作当前模型拟合数据的残差（residual）。所以，对回归问题的提升树算法来说，只需拟合当前模型的残差。</p>
<p><strong>算法步骤：</strong></p>
<p>（1）初始化<span class="math inline">\(f_0(x)=0\)</span>；</p>
<p>（2）对<span class="math inline">\(m=1,2,\cdots ,M\)</span>：（相当于M次循环）</p>
<p>​ （a）计算残差： <span class="math display">\[
r_mi=y_i-f_{m-1}(x_i),\quad i=1,2, \cdots, N
\]</span> ​ （b）拟合残差<span class="math inline">\(r_mi\)</span>学习一个回归树，得到<span class="math inline">\(T(x ; \Theta)\)</span>。</p>
<p>​ （c）更新<span class="math inline">\(f_{m}(x)=f_{m-1}(x)+T\left(x ; \Theta_{m}\right)\)</span>​。</p>
<p>（3）得到回归问题提升树 <span class="math display">\[
f_{M}(x)=\sum_{m=1}^{M} T\left(x ; \Theta_{m}\right)
\]</span> 提升树的算法举例详见统计学习方法P168</p>
<h3 id="梯度提升">3.梯度提升</h3>
<p>这里有个很关键的点需要说明，梯度提升不是梯度上升，梯度提升中的“提升”指的是提升树，梯度提升这个词指的是在提升树算法中运用梯度。而梯度上升指的是需要优化的目标函数取值的上升，这两者是完全不同的概念。当损失函数是平方损失函数和指数损失函数时，优化比较容易，但对于一般损失函数而言，往往并不容易。梯度提升，就是利用损失函数负梯度作为回归问题提升树算法中的残差近似值，拟合一个回归树。</p>
<p>算法步骤：</p>
<p>（1）初始化 <span class="math display">\[
f_{0}(x)=\arg \min _{c} \sum_{i=1}^{N} L\left(y_{i}, c\right)
\]</span> 在最小二乘回归树中，当<span class="math inline">\(c\)</span>取得每个单元上的均值时，平方误差损失函数最小，但此处是一般损失函数，不能直接取均值。</p>
<p>（2）对<span class="math inline">\(m=1,2,\cdots ,M\)</span>：（相当于M次循环）</p>
<p>​ （a）对<span class="math inline">\(i=1,2,\cdots ,N\)</span>​​： <span class="math display">\[
r_{m i}=-\left[\frac{\partial L\left(y_{i}, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f_{m-1}(x)}
\]</span> 对于平方损失函数，此处的负梯度就是通常说的残差；对于一般损失函数，负梯度是残差的近似值。</p>
<p>​ （b）对<span class="math inline">\(r_{mi}\)</span>拟合一个回归树，得到第<span class="math inline">\(m\)</span>课树的叶结点区域<span class="math inline">\(R_{mj},j=1,2,\cdots,J\)</span>。</p>
<p>​ （c）对<span class="math inline">\(j=1,2,\cdots,J\)</span>，计算 <span class="math display">\[
c_{m j}=\arg \min _{c} \sum_{x_{i} \in R_{m j}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+c\right)
\]</span> ​ （d）更新<span class="math inline">\(f_{m}(x)=f_{m-1}(x)+\sum_{j=1}^{J} c_{m j} I\left(x \in R_{m j}\right)\)</span>​</p>
<p>（3）得到回归树 <span class="math display">\[
\hat{f}(x)=f_{M}(x)=\sum_{m=1}^{M} \sum_{j=1}^{J} c_{m j} I\left(x \in R_{m j}\right)
\]</span> 其实梯度提升算法和提升树算法没有很大的不同，不同之处仅仅在于用负梯度取代了残差进行计算，更具有一般性。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"> <i class="fa fa-tag"></i> 机器学习</a>
              <a href="/tags/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" rel="tag"> <i class="fa fa-tag"></i> 优化方法</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/08/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%E2%80%94%E2%80%94%E5%86%B3%E7%AD%96%E6%A0%91/" rel="prev" title="机器学习总结——决策树">
                  <i class="fa fa-chevron-left"></i> 机器学习总结——决策树
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/03/09/%E6%9C%8D%E5%8A%A1%E5%99%A8Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" rel="next" title="服务器Linux常用命令（持续更新）">
                  服务器Linux常用命令（持续更新） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
    
     <div><center>
     <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
     <div class="social-share" data-sites="weibo, qq, qzone,wechat"></div>
     <script src="http://apps.bdimg.com/libs/jquery/1.8.2/jquery.js"></script>
     <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
  </center></div>  
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-chevron-circle-right"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Rex</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">34k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">31 分钟</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("07/22/2021 17:00:40");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已运行 "+dnum+" 天 "+hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    </div>
  </footer>

  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>

<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>




  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
